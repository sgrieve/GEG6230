<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Stuart Grieve" />
  <meta name="date" content="2019-03-11" />
  <title>GEG5223 Practical 9</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--
  https://gist.github.com/killercup/5917178
  -->
  <style type="text/css">
  html {
    font-size: 100%;
    overflow-y: scroll;
    -webkit-text-size-adjust: 100%;
    -ms-text-size-adjust: 100%;
  }
  
  body {
    color: #444;
    font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
    font-size: 12px;
    line-height: 1.7;
    padding: 1em;
    margin: auto;
    max-width: 42em;
    background: #fefefe;
  }
  
  a {
    color: #0645ad;
    text-decoration: none;
  }
  
  a:visited {
    color: #0b0080;
  }
  
  a:hover {
    color: #06e;
  }
  
  a:active {
    color: #faa700;
  }
  
  a:focus {
    outline: thin dotted;
  }
  
  *::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
  }
  
  *::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #000;
  }
  
  a::-moz-selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
  }
  
  a::selection {
    background: rgba(255, 255, 0, 0.3);
    color: #0645ad;
  }
  
  p {
    margin: 1em 0;
  }
  
  img {
    max-width: 100%;
  }
  
  h1, h2, h3, h4, h5, h6 {
    color: #111;
    line-height: 125%;
    margin-top: 2em;
    font-weight: normal;
  }
  
  h4, h5, h6 {
    font-weight: bold;
  }
  
  h1 {
    font-size: 2.5em;
  }
  
  h2 {
    font-size: 2em;
  }
  
  h3 {
    font-size: 1.5em;
  }
  
  h4 {
    font-size: 1.2em;
  }
  
  h5 {
    font-size: 1em;
  }
  
  h6 {
    font-size: 0.9em;
  }
  
  blockquote {
    color: #666666;
    margin: 0;
    padding-left: 3em;
    border-left: 0.5em #EEE solid;
  }
  
  hr {
    display: block;
    height: 2px;
    border: 0;
    border-top: 1px solid #aaa;
    border-bottom: 1px solid #eee;
    margin: 1em 0;
    padding: 0;
  }
  
  pre, code, kbd, samp {
    color: #000;
    font-family: monospace, monospace;
    _font-family: 'courier new', monospace;
    font-size: 0.98em;
  }
  
  pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  b, strong {
    font-weight: bold;
  }
  
  dfn {
    font-style: italic;
  }
  
  ins {
    background: #ff9;
    color: #000;
    text-decoration: none;
  }
  
  mark {
    background: #ff0;
    color: #000;
    font-style: italic;
    font-weight: bold;
  }
  
  sub, sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
  }
  
  sup {
    top: -0.5em;
  }
  
  sub {
    bottom: -0.25em;
  }
  
  ul, ol {
    margin: 1em 0;
    padding: 0 0 0 2em;
  }
  
  li p:last-child {
    margin-bottom: 0;
  }
  
  ul ul, ol ol {
    margin: .3em 0;
  }
  
  dl {
    margin-bottom: 1em;
  }
  
  dt {
    font-weight: bold;
    margin-bottom: .8em;
  }
  
  dd {
    margin: 0 0 .8em 2em;
  }
  
  dd:last-child {
    margin-bottom: 0;
  }
  
  img {
    border: 0;
    -ms-interpolation-mode: bicubic;
    vertical-align: middle;
  }
  
  figure {
    display: block;
    text-align: center;
    margin: 1em 0;
  }
  
  figure img {
    border: none;
    margin: 0 auto;
  }
  
  figcaption {
    font-size: 0.8em;
    font-style: italic;
    margin: 0 0 .8em;
  }
  
  table {
    margin-bottom: 2em;
    border-bottom: 1px solid #ddd;
    border-right: 1px solid #ddd;
    border-spacing: 0;
    border-collapse: collapse;
  }
  
  table th {
    padding: .2em 1em;
    background-color: #eee;
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
  }
  
  table td {
    padding: .2em 1em;
    border-top: 1px solid #ddd;
    border-left: 1px solid #ddd;
    vertical-align: top;
  }
  
  .author {
    font-size: 1.2em;
    text-align: center;
  }
  
  @media only screen and (min-width: 480px) {
    body {
      font-size: 14px;
    }
  }
  @media only screen and (min-width: 768px) {
    body {
      font-size: 16px;
    }
  }
  @media print {
    * {
      background: transparent !important;
      color: black !important;
      filter: none !important;
      -ms-filter: none !important;
    }
  
    body {
      font-size: 12pt;
      max-width: 100%;
    }
  
    a, a:visited {
      text-decoration: underline;
    }
  
    hr {
      height: 1px;
      border: 0;
      border-bottom: 1px solid black;
    }
  
    a[href]:after {
      content: " (" attr(href) ")";
    }
  
    abbr[title]:after {
      content: " (" attr(title) ")";
    }
  
    .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after {
      content: "";
    }
  
    pre, blockquote {
      border: 1px solid #999;
      padding-right: 1em;
      page-break-inside: avoid;
    }
  
    tr, img {
      page-break-inside: avoid;
    }
  
    img {
      max-width: 100% !important;
    }
  
    @page :left {
      margin: 15mm 20mm 15mm 10mm;
  }
  
    @page :right {
      margin: 15mm 10mm 15mm 20mm;
  }
  
    p, h2, h3 {
      orphans: 3;
      widows: 3;
    }
  
    h2, h3 {
      page-break-after: avoid;
    }
  }
  </style>
</head>
<body>
<div id="header">
<h1 class="title">GEG5223 Practical 9</h1>
<h2 class="author">Stuart Grieve</h2>
<h3 class="date">03/11/19</h3>
</div>
<h1 id="interpolation">Interpolation</h1>
<h2 id="aims-and-objectives">Aims and Objectives</h2>
<p>In this practical you will be using some lidar point data from OpenTopography to explore a range of interpolation techniques, which were discussed in the lecture. We will also look at ways of evaluating the quality of these interpolations.</p>
<p>By the end of this class you should be able to:</p>
<ol style="list-style-type: decimal">
<li>Perform interpolation using the Thiessen polygon method</li>
<li>Perform interpolation using the IDW method</li>
<li>Perform interpolation using the spline method</li>
<li>Quantify the quality of each of these techniques</li>
</ol>
<h2 id="data">Data</h2>
<p>The data we are using today are two shapefiles of elevation values generated from topographic data provided by https://OpenTopography.org for the San Bernadino Mountains in California. The two shapefiles cover the same spatial area, one will be used for our interpolations, and the other will be used to evaluate those interpolations.</p>
<p>Note that in reality topographic data is rarely stored as point shapefiles due to the inefficiency of the file format. We have converted the data into this format as it is easier to use for ArcGIS's interpolation tools. Advanced Geospatial Science will return to this topic and look at other ways of interpolating topographic data.</p>
<p>Download the file <code>practical_9.zip</code> from QMplus and extract it into a sensible location (either a folder or a GeoDatabase). The zip archive contains:</p>
<ul>
<li><code>training_data.shp</code> - A shapefile containing point elevation values for our study area.</li>
<li><code>test_data.shp</code> - A shapefile containing point elevation values for our study area, which can be used to evaluate our interpolated surface.</li>
<li><code>hillshade.tif</code> - A hillshade of a larger area of the San Bernadino Mountains, that can be used to obtain some context for the data we are studying.</li>
</ul>
<h3 id="visualising-the-data">Visualising the data</h3>
<p>Load the <code>training_data.shp</code> and <code>hillshade.tif</code> using the <code>Add Data</code> button. You can see that we have over 8000 lidar elevation values unevenly distributed across part of the landscape. Open the attribute table for <code>training_data.shp</code> and you will see that every point has an elevation value (in meters above sea level) recorded as an attribute. Note that the <code>Shape*</code> column describes the data as <code>Point ZM</code>, this means that the elevation values are also stored as part of the shapefile geometry. Unfortunately, some of the tools we will be using today do not support true three dimensional shapefiles, so we have the elevation values as an attribute for those cases.</p>
<p>We are first going to visualise the point data, by colouring the points by their elevation values. Open the <code>training_data.shp</code> <code>Properties</code> window by right clicking on the layer in the table of contents. Select the <code>Symbology</code> tab and choose <code>Quantities</code>. From here we can select the values we wish to display (Elevation in this case), choose an appropriate colour ramp and number of classes. The more classes you choose the smoother the transitions between colours will be.</p>
<p><img src="../../img/point_quant.png" alt="Point Properties" /> <!-- .element width="80%" --></p>
<p>This helps us to see that there is a large ridge running across our dataset from east to west.</p>
<h3 id="thiessen-polygons">Thiessen polygons</h3>
<p>Our first interpolation will use Thiessen polygons to create polygons of constant elevation values around each of our points. To do this we will open the <code>Create Thiessen Polygons</code> tool, which can be found under <code>ArcToolBox &gt; Analysis Tools &gt; Proximity &gt; Create Thiessen Polygons</code>. There are only three parameters for this tool, the input data is our <code>training_data.shp</code>, the <code>Output Feature Class</code> is the path and filename of the output Thiessen polygons, and we <strong>must</strong> set <code>Output Fields</code> to <code>ALL</code>, otherwise the elevation data will not be stored in each polygon. Once the tool has completed, it will load a new polygon dataset.</p>
<p>Zoom in to some of the data, and look at how each polygon has been 'grown' around each point. Does this look similar to what you saw in the lecture?</p>
<p>Lets use the same visualisation techniques as we did for the points to assign a colour to each polygon based on it's elevation value.</p>
<p><img src="../../img/thiessen_example.png" alt="Coloured Thiessen polygons" /> <!-- .element width="80%" --></p>
<p>Note that at the edges things always go a little odd for Thiessen polygons, so we are going to crop the edges so that we are only <strong>interpolating</strong> and not <strong>extrapolating</strong>. To do this we need to create a polygon around the outermost points in our dataset. This is known as a <strong>convex hull</strong>. Open the <code>Minimum Bounding Geometry</code> tool from <code>ArcToolBox &gt; Data Management Tools &gt; Features &gt; Minimum Bounding Geometry</code> and fill in the following parameters:</p>
<ul>
<li><code>Input Features</code>: This is our training data</li>
<li><code>Output Feature Class</code>: This is the path and filename of our output file</li>
<li><code>Geometry Type</code>: Set this to <code>CONVEX_HULL</code></li>
<li>Everything else can be left as the default</li>
</ul>
<p>Once this tool has run, we will have a polygon dataset which outlines the limits of the interpolation we can perform. We can now use the <code>Intersect</code> tool, found in <code>ArcToolBox &gt; Analysis Tools &gt; Overlay &gt; Intersect</code> to remove the bad data from the edges of our dataset, using the following parameters:</p>
<ul>
<li><code>Input Features</code>: Select both the generated convex hull and the thiessen polygon datasets</li>
<li><code>Output Feature Class</code>: This is the path and filename of our output file</li>
<li>Everything else can be left as the default</li>
</ul>
<p>Now, we want to convert these clipped polygons into a surface, using the <code>Polygon to Raster</code> tool, found in <code>ArcToolBox &gt; Conversion &gt; To Raster &gt; Polygon to Raster</code>. Use the following parameters:</p>
<ul>
<li><code>Input Features</code>: This is our clipped Thiessen polygons</li>
<li><code>Value field</code>: Choose <code>elevation</code></li>
<li><code>Output Feature Class</code>: This is the path and filename of our output raster</li>
<li>Everything else can be left as the default</li>
</ul>
<p>Finally, create a hillshade of this new raster surface, so that we can compare it qualitatively to the provided hillshade of the whole landscape. <strong>What are your first impressions? Is this a good way of interpolating elevation values? Why?</strong></p>
<h3 id="inverse-distance-weighting-idw">Inverse Distance Weighting (IDW)</h3>
<p>We are now going to revisit the IDW tool that we used briefly last week, now that we better understand what it is doing. The tool can be found in <code>ArcToolbox &gt; Spatial Analyst Tools &gt; Interpolation &gt; IDW</code>. The standard inputs are as follows:</p>
<ul>
<li><code>Input point features</code>: The training data</li>
<li><code>Z value field</code>: <code>elevation</code></li>
<li><code>Output raster</code>: The path and filename of the interpolated raster file</li>
<li><code>Output cell size</code>: This is the size of each cell in our output raster grid. Leave this as the default.</li>
</ul>
<p>There are then a series of settings which we can adjust, which will alter the nature of the interpolated surface we create. The <code>Power</code> option, reflects the exponent on the distance value:</p>
<p><span class="math display">\[ Z_p = \frac{\sum_{i=1}^{n} \frac{Z_i}{d^2}}{\sum_{i=1}^{n} \frac{1}{d^2}}\]</span></p>
<p>In the lecture we saw that setting <code>Power</code> to <code>2</code> would preserve peaks in the surface, while using a value of <code>1</code> would smooth out the data.</p>
<p>The next parameter is the <code>Search radius</code>, which can be either <code>Variable</code> or <code>Fixed</code>. This controls the neighbourhood that we use to calculated the elevation value for each cell in our new raster grid. If we select <code>variable</code>, we can then select a number of points to be included in each IDW calculation. This means that the neighbourhood is smaller in areas of high point density and larger in less dense areas.</p>
<p>If we select <code>Fixed</code>, we can specify a <code>Distance</code>, which is the radius of the neighbourhood to be used across the whole dataset. If we choose a large value (100s of meters) we will include lots of points, smoothing out any local trends, but if we choose a much smaller value we will not have many points, resulting in a rougher surface.</p>
<p>Note that by default ArcMap applies a classified colour ramp to the outputs of our interpolations. It will be easier for you to interpret these surfaces by changing the symbology from <code>classified</code> to <code>stretched</code> in the layer's properties, in addition to generating a hillshade of each new surface.</p>
<p>Using all of these settings, create a series of different interpolated surfaces to explore how changing these parameters impacts the final result. Decide what set of parameters makes the best interpolation, when compared to <code>hillshade.tif</code>.</p>
<p><strong>It is important that you record what parameters you used for this chosen surface, as this information may be needed in your portfolio write-up.</strong></p>
<h3 id="splines">Splines</h3>
<p>Reminder from the lecture:</p>
<p>Regularised splines: - Smoother surface - High weight leads to smooth surface, ignoring more points</p>
<p>Tension splines: - Rougher surface - High weight creates 'stiffer' surface that passes through points more closely</p>
<p>The final interpolation method we will be using is the <code>Spline</code> tool, found in <code>ArcToolbox &gt; Spatial Analyst Tools &gt; Interpolation &gt; Spline</code>, which has the following parameters:</p>
<ul>
<li><code>Input point features</code>: The training data</li>
<li><code>Z value field</code>: <code>elevation</code></li>
<li><code>Output raster</code>: The path and filename of the interpolated raster file</li>
<li><code>Output cell size</code>: This is the size of each cell in our output raster grid. Leave this as the default.</li>
<li><code>Spline type</code>: Either <code>REGULARIZED</code> or <code>TENSION</code></li>
<li><code>Weight</code>: a value ranging from 0.01 to &gt; 50</li>
<li><code>Number of points</code>: The number of points included in each neighbourhood calculation. Default is 12.</li>
</ul>
<p>As with the IDW interpolations performed in the last section, run several Spline interpolations, exploring how changing the spline type from <code>REGULARIZED</code> to <code>TENSION</code> and adjusting the weight and number of points impacts the resulting interpolated surface.</p>
<p><strong>It is important that you record what parameters you used for this chosen surface, as this information may be needed in your portfolio write-up.</strong></p>
<h3 id="evaluating-interpolations">Evaluating interpolations</h3>
<p>In addition to the qualitative comparisons we have been making between the hillshaded interpolations, we can also employ some of the quantitative approaches to evaluation we discussed in the last lecture.</p>
<p>To perform a quantitative evaluation we can now load our test data (<code>test_data.shp</code>) that we have so far not used in our analysis, and so we know it is independent of what we have done so far.</p>
<p>To do this we are going to use the <code>Sample</code> tool, which we used in Practical 6 to generate river long profiles. It can be found in <code>ArcToolbox &gt; Spatial Analyst Tools &gt; Extraction &gt; Sample</code>. This tool will extract the values of the given raster datasets and needs the following inputs:</p>
<ul>
<li><code>Input rasters</code>: These are the values we will be sampling, so add our three interpolated rasters, one for each of our interpolation methods.</li>
<li><code>Input location raster or point features</code>: This is our test data.</li>
<li><code>Output table</code>: This is the filename of the output data.</li>
<li>The other parameters do not need to be changed</li>
</ul>
<p>Unlike last time we used this tool, there is one more step we need to perform before we can load our data into Excel. We need to join the attributes of <code>test_data.shp</code> into the table we have just created, so that we have the actual elevation values in the same place as the elevations from our three interpolations.</p>
<p>Right click on the table created by the <code>Sample</code> tool, and select <code>Joins and Relates &gt; Join</code>:</p>
<p><img src="../../img/join_1.png" alt="Join options" /> <!-- .element width="80%" --></p>
<p>Complete the options as shown in the image:</p>
<p><img src="../../img/join_2.png" alt="Join options" /> <!-- .element width="80%" --></p>
<p>which will link the attributes to the new table based on the unique <code>FID</code> field within <code>test_data.shp</code>. After this has completed, open the attribute table and check that you now have a table containing the elevation values from <code>test_data.shp</code> alongside the 3 interpolated elevation values.</p>
<p><img src="../../img/join_3.png" alt="Join options" /> <!-- .element width="80%" --></p>
<p>Right click on the joined table in the Table of Contents, and select <code>Data &gt; Export</code>. Give a sensible filename to the exported data and <strong>Make sure that you give this file the <code>.dbf</code> extension, or it will not load into Excel!</strong></p>
<p>Load this file in Excel, and save it as a proper Excel workbook, to ensure that you do not lose any data. The we now need to calculate our residuals, this is done by taking the interpolated measurement and subtracting the elevation from <code>test_data.shp</code> from it. Create three new columns your residuals for each interpolation type.</p>
<p>Once you have each column of residuals, you can plot them as a histogram using the histogram tool:</p>
<p><img src="../../img/excel_hist.png" alt="Excel histogram" /> <!-- .element width="80%" --></p>
<p><strong>What patterns can you see in these histograms? Are the errors the same for each interpolation method? Are they biased towards over or under estimates?</strong></p>
<h2 id="portfolio-instructions">Portfolio instructions</h2>
<ul>
<li>Create a histogram of residuals for each of the three interpolation methods</li>
<li>In 200 words, explain which of the three interpolation methods you deem to be most appropriate for the given data, and why?</li>
</ul>
</body>
</html>
